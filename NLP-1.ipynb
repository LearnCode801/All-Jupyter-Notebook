{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7abb710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in e:\\anaconda\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in e:\\anaconda\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\anaconda\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: joblib in e:\\anaconda\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in e:\\anaconda\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in e:\\anaconda\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce5147bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg=\"\"\"The second vision, known as the connectionist approach, sought to achieve intelligence through learning. Proponents of this approach, most prominently Frank Rosenblatt, sought to connect Perceptron in ways inspired by connections of neurons.[22] James Manyika and others have compared the two approaches to the mind (Symbolic AI) and the brain (connectionist). Manyika argues that symbolic approaches dominated the push for artificial intelligence in this period, due in part to its connection to intellectual traditions of Descartes, Boole, Gottlob Frege, Bertrand Russell, and others. Connectionist approaches based on cybernetics or artificial neural networks were pushed to the background but have gained new prominence in recent decades.[23]\n",
    "\n",
    "The field of AI research was born at a workshop at Dartmouth College in 1956.[d][26] The attendees became the founders and leaders of AI research.[e] They and their students produced programs that the press described as \"astonishing\":[f] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.[g][28]\n",
    "\n",
    "By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense[29] and laboratories had been established around the world.[30]\n",
    "\n",
    "Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field.[31] Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\".[32] Marvin Minsky agreed, writing, \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\".[33]\n",
    "\n",
    "They had failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill[34] and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an \"AI winter\", a period when obtaining funding for AI projects was difficult.[6]\n",
    "\n",
    "In the early 1980s, AI research was revived by the commercial success of expert systems,[35] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research.[5] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.[7]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "50bdd286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8083b044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Muhammad Talha\n",
      "[nltk_data]     Awan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# tokenization \n",
    "nltk.download('punkt')\n",
    "sentences=nltk.sent_tokenize(pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "68df474f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The second vision, known as the connectionist approach, sought to achieve intelligence through learning.',\n",
       " 'Proponents of this approach, most prominently Frank Rosenblatt, sought to connect Perceptron in ways inspired by connections of neurons.',\n",
       " '[22] James Manyika and others have compared the two approaches to the mind (Symbolic AI) and the brain (connectionist).',\n",
       " 'Manyika argues that symbolic approaches dominated the push for artificial intelligence in this period, due in part to its connection to intellectual traditions of Descartes, Boole, Gottlob Frege, Bertrand Russell, and others.',\n",
       " 'Connectionist approaches based on cybernetics or artificial neural networks were pushed to the background but have gained new prominence in recent decades.',\n",
       " '[23]\\n\\nThe field of AI research was born at a workshop at Dartmouth College in 1956.',\n",
       " '[d][26] The attendees became the founders and leaders of AI research.',\n",
       " '[e] They and their students produced programs that the press described as \"astonishing\":[f] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.',\n",
       " '[g][28]\\n\\nBy the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense[29] and laboratories had been established around the world.',\n",
       " '[30]\\n\\nResearchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field.',\n",
       " '[31] Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\".',\n",
       " '[32] Marvin Minsky agreed, writing, \"within a generation ... the problem of creating \\'artificial intelligence\\' will substantially be solved\".',\n",
       " '[33]\\n\\nThey had failed to recognize the difficulty of some of the remaining tasks.',\n",
       " 'Progress slowed and in 1974, in response to the criticism of Sir James Lighthill[34] and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI.',\n",
       " 'The next few years would later be called an \"AI winter\", a period when obtaining funding for AI projects was difficult.',\n",
       " '[6]\\n\\nIn the early 1980s, AI research was revived by the commercial success of expert systems,[35] a form of AI program that simulated the knowledge and analytical skills of human experts.',\n",
       " 'By 1985, the market for AI had reached over a billion dollars.',\n",
       " \"At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research.\",\n",
       " '[5] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.',\n",
       " '[7]']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d53872b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4f4a588f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'histori'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying the stemmer at the one word\n",
    "stemmer=PorterStemmer()\n",
    "stemmer.stem('history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e8922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "81a58515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Muhammad Talha\n",
      "[nltk_data]     Awan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Muhammad Talha\n",
      "[nltk_data]     Awan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'going'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Applying the Lemmatizer at the one word\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('going')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dcfc9b69",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "The second vision, known as the connectionist approach, sought to achieve intelligence through learning.\n",
      "the second vision  known as the connectionist approach  sought to achieve intelligence through learning \n",
      "1\n",
      "Proponents of this approach, most prominently Frank Rosenblatt, sought to connect Perceptron in ways inspired by connections of neurons.\n",
      "proponents of this approach  most prominently frank rosenblatt  sought to connect perceptron in ways inspired by connections of neurons \n",
      "2\n",
      "[22] James Manyika and others have compared the two approaches to the mind (Symbolic AI) and the brain (connectionist).\n",
      "     james manyika and others have compared the two approaches to the mind  symbolic ai  and the brain  connectionist  \n",
      "3\n",
      "Manyika argues that symbolic approaches dominated the push for artificial intelligence in this period, due in part to its connection to intellectual traditions of Descartes, Boole, Gottlob Frege, Bertrand Russell, and others.\n",
      "manyika argues that symbolic approaches dominated the push for artificial intelligence in this period  due in part to its connection to intellectual traditions of descartes  boole  gottlob frege  bertrand russell  and others \n",
      "4\n",
      "Connectionist approaches based on cybernetics or artificial neural networks were pushed to the background but have gained new prominence in recent decades.\n",
      "connectionist approaches based on cybernetics or artificial neural networks were pushed to the background but have gained new prominence in recent decades \n",
      "5\n",
      "[23]\n",
      "\n",
      "The field of AI research was born at a workshop at Dartmouth College in 1956.\n",
      "      the field of ai research was born at a workshop at dartmouth college in      \n",
      "6\n",
      "[d][26] The attendees became the founders and leaders of AI research.\n",
      " d      the attendees became the founders and leaders of ai research \n",
      "7\n",
      "[e] They and their students produced programs that the press described as \"astonishing\":[f] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.\n",
      " e  they and their students produced programs that the press described as  astonishing   f  computers were learning checkers strategies  solving word problems in algebra  proving logical theorems and speaking english \n",
      "8\n",
      "[g][28]\n",
      "\n",
      "By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense[29] and laboratories had been established around the world.\n",
      " g       by the middle of the     s  research in the u s  was heavily funded by the department of defense     and laboratories had been established around the world \n",
      "9\n",
      "[30]\n",
      "\n",
      "Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field.\n",
      "      researchers in the     s and the     s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field \n",
      "10\n",
      "[31] Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\".\n",
      "     herbert simon predicted   machines will be capable  within twenty years  of doing any work a man can do  \n",
      "11\n",
      "[32] Marvin Minsky agreed, writing, \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\".\n",
      "     marvin minsky agreed  writing   within a generation     the problem of creating  artificial intelligence  will substantially be solved  \n",
      "12\n",
      "[33]\n",
      "\n",
      "They had failed to recognize the difficulty of some of the remaining tasks.\n",
      "      they had failed to recognize the difficulty of some of the remaining tasks \n",
      "13\n",
      "Progress slowed and in 1974, in response to the criticism of Sir James Lighthill[34] and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI.\n",
      "progress slowed and in       in response to the criticism of sir james lighthill     and ongoing pressure from the us congress to fund more productive projects  both the u s  and british governments cut off exploratory research in ai \n",
      "14\n",
      "The next few years would later be called an \"AI winter\", a period when obtaining funding for AI projects was difficult.\n",
      "the next few years would later be called an  ai winter   a period when obtaining funding for ai projects was difficult \n",
      "15\n",
      "[6]\n",
      "\n",
      "In the early 1980s, AI research was revived by the commercial success of expert systems,[35] a form of AI program that simulated the knowledge and analytical skills of human experts.\n",
      "     in the early     s  ai research was revived by the commercial success of expert systems      a form of ai program that simulated the knowledge and analytical skills of human experts \n",
      "16\n",
      "By 1985, the market for AI had reached over a billion dollars.\n",
      "by       the market for ai had reached over a billion dollars \n",
      "17\n",
      "At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research.\n",
      "at the same time  japan s fifth generation computer project inspired the u s  and british governments to restore funding for academic research \n",
      "18\n",
      "[5] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.\n",
      "    however  beginning with the collapse of the lisp machine market in       ai once again fell into disrepute  and a second  longer lasting winter began \n",
      "19\n",
      "[7]\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "# Regularization to the corpus\n",
    "\n",
    "import re\n",
    "corpus=[]\n",
    "for i in range (len(sent)):\n",
    "    print(i)\n",
    "    print(sentences[i])\n",
    "    review=re.sub('[^a-zA-Z]',' ',sentences[i])\n",
    "    review=review.lower()\n",
    "    print(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8707f189",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second\n",
      "vision\n",
      "known\n",
      "connectionist\n",
      "approach\n",
      "sought\n",
      "achiev\n",
      "intellig\n",
      "learn\n",
      "propon\n",
      "approach\n",
      "promin\n",
      "frank\n",
      "rosenblatt\n",
      "sought\n",
      "connect\n",
      "perceptron\n",
      "way\n",
      "inspir\n",
      "connect\n",
      "neuron\n",
      "jame\n",
      "manyika\n",
      "other\n",
      "compar\n",
      "two\n",
      "approach\n",
      "mind\n",
      "symbol\n",
      "ai\n",
      "brain\n",
      "connectionist\n",
      "manyika\n",
      "argu\n",
      "symbol\n",
      "approach\n",
      "domin\n",
      "push\n",
      "artifici\n",
      "intellig\n",
      "period\n",
      "due\n",
      "part\n",
      "connect\n",
      "intellectu\n",
      "tradit\n",
      "descart\n",
      "bool\n",
      "gottlob\n",
      "frege\n",
      "bertrand\n",
      "russel\n",
      "other\n",
      "connectionist\n",
      "approach\n",
      "base\n",
      "cybernet\n",
      "artifici\n",
      "neural\n",
      "network\n",
      "push\n",
      "background\n",
      "gain\n",
      "new\n",
      "promin\n",
      "recent\n",
      "decad\n",
      "field\n",
      "ai\n",
      "research\n",
      "born\n",
      "workshop\n",
      "dartmouth\n",
      "colleg\n",
      "attende\n",
      "becam\n",
      "founder\n",
      "leader\n",
      "ai\n",
      "research\n",
      "e\n",
      "student\n",
      "produc\n",
      "program\n",
      "press\n",
      "describ\n",
      "astonish\n",
      "f\n",
      "comput\n",
      "learn\n",
      "checker\n",
      "strategi\n",
      "solv\n",
      "word\n",
      "problem\n",
      "algebra\n",
      "prove\n",
      "logic\n",
      "theorem\n",
      "speak\n",
      "english\n",
      "g\n",
      "middl\n",
      "research\n",
      "u\n",
      "heavili\n",
      "fund\n",
      "depart\n",
      "defens\n",
      "laboratori\n",
      "establish\n",
      "around\n",
      "world\n",
      "research\n",
      "convinc\n",
      "symbol\n",
      "approach\n",
      "would\n",
      "eventu\n",
      "succeed\n",
      "creat\n",
      "machin\n",
      "artifici\n",
      "gener\n",
      "intellig\n",
      "consid\n",
      "goal\n",
      "field\n",
      "herbert\n",
      "simon\n",
      "predict\n",
      "machin\n",
      "capabl\n",
      "within\n",
      "twenti\n",
      "year\n",
      "work\n",
      "man\n",
      "marvin\n",
      "minski\n",
      "agre\n",
      "write\n",
      "within\n",
      "gener\n",
      "problem\n",
      "creat\n",
      "artifici\n",
      "intellig\n",
      "substanti\n",
      "solv\n",
      "fail\n",
      "recogn\n",
      "difficulti\n",
      "remain\n",
      "task\n",
      "progress\n",
      "slow\n",
      "respons\n",
      "critic\n",
      "sir\n",
      "jame\n",
      "lighthil\n",
      "ongo\n",
      "pressur\n",
      "us\n",
      "congress\n",
      "fund\n",
      "product\n",
      "project\n",
      "u\n",
      "british\n",
      "govern\n",
      "cut\n",
      "exploratori\n",
      "research\n",
      "ai\n",
      "next\n",
      "year\n",
      "would\n",
      "later\n",
      "call\n",
      "ai\n",
      "winter\n",
      "period\n",
      "obtain\n",
      "fund\n",
      "ai\n",
      "project\n",
      "difficult\n",
      "earli\n",
      "ai\n",
      "research\n",
      "reviv\n",
      "commerci\n",
      "success\n",
      "expert\n",
      "system\n",
      "form\n",
      "ai\n",
      "program\n",
      "simul\n",
      "knowledg\n",
      "analyt\n",
      "skill\n",
      "human\n",
      "expert\n",
      "market\n",
      "ai\n",
      "reach\n",
      "billion\n",
      "dollar\n",
      "time\n",
      "japan\n",
      "fifth\n",
      "gener\n",
      "comput\n",
      "project\n",
      "inspir\n",
      "u\n",
      "british\n",
      "govern\n",
      "restor\n",
      "fund\n",
      "academ\n",
      "research\n",
      "howev\n",
      "begin\n",
      "collaps\n",
      "lisp\n",
      "machin\n",
      "market\n",
      "ai\n",
      "fell\n",
      "disreput\n",
      "second\n",
      "longer\n",
      "last\n",
      "winter\n",
      "began\n"
     ]
    }
   ],
   "source": [
    "#stemmer\n",
    "for i in (corpus):\n",
    "    words = nltk.word_tokenize(i)\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            print(stemmer.stem(word))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "12bfbfc3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second\n",
      "vision\n",
      "known\n",
      "connectionist\n",
      "approach\n",
      "sought\n",
      "achieve\n",
      "intelligence\n",
      "learning\n",
      "proponent\n",
      "approach\n",
      "prominently\n",
      "frank\n",
      "rosenblatt\n",
      "sought\n",
      "connect\n",
      "perceptron\n",
      "way\n",
      "inspired\n",
      "connection\n",
      "neuron\n",
      "james\n",
      "manyika\n",
      "others\n",
      "compared\n",
      "two\n",
      "approach\n",
      "mind\n",
      "symbolic\n",
      "ai\n",
      "brain\n",
      "connectionist\n",
      "manyika\n",
      "argues\n",
      "symbolic\n",
      "approach\n",
      "dominated\n",
      "push\n",
      "artificial\n",
      "intelligence\n",
      "period\n",
      "due\n",
      "part\n",
      "connection\n",
      "intellectual\n",
      "tradition\n",
      "descartes\n",
      "boole\n",
      "gottlob\n",
      "frege\n",
      "bertrand\n",
      "russell\n",
      "others\n",
      "connectionist\n",
      "approach\n",
      "based\n",
      "cybernetics\n",
      "artificial\n",
      "neural\n",
      "network\n",
      "pushed\n",
      "background\n",
      "gained\n",
      "new\n",
      "prominence\n",
      "recent\n",
      "decade\n",
      "field\n",
      "ai\n",
      "research\n",
      "born\n",
      "workshop\n",
      "dartmouth\n",
      "college\n",
      "attendee\n",
      "became\n",
      "founder\n",
      "leader\n",
      "ai\n",
      "research\n",
      "e\n",
      "student\n",
      "produced\n",
      "program\n",
      "press\n",
      "described\n",
      "astonishing\n",
      "f\n",
      "computer\n",
      "learning\n",
      "checker\n",
      "strategy\n",
      "solving\n",
      "word\n",
      "problem\n",
      "algebra\n",
      "proving\n",
      "logical\n",
      "theorem\n",
      "speaking\n",
      "english\n",
      "g\n",
      "middle\n",
      "research\n",
      "u\n",
      "heavily\n",
      "funded\n",
      "department\n",
      "defense\n",
      "laboratory\n",
      "established\n",
      "around\n",
      "world\n",
      "researcher\n",
      "convinced\n",
      "symbolic\n",
      "approach\n",
      "would\n",
      "eventually\n",
      "succeed\n",
      "creating\n",
      "machine\n",
      "artificial\n",
      "general\n",
      "intelligence\n",
      "considered\n",
      "goal\n",
      "field\n",
      "herbert\n",
      "simon\n",
      "predicted\n",
      "machine\n",
      "capable\n",
      "within\n",
      "twenty\n",
      "year\n",
      "work\n",
      "man\n",
      "marvin\n",
      "minsky\n",
      "agreed\n",
      "writing\n",
      "within\n",
      "generation\n",
      "problem\n",
      "creating\n",
      "artificial\n",
      "intelligence\n",
      "substantially\n",
      "solved\n",
      "failed\n",
      "recognize\n",
      "difficulty\n",
      "remaining\n",
      "task\n",
      "progress\n",
      "slowed\n",
      "response\n",
      "criticism\n",
      "sir\n",
      "james\n",
      "lighthill\n",
      "ongoing\n",
      "pressure\n",
      "u\n",
      "congress\n",
      "fund\n",
      "productive\n",
      "project\n",
      "u\n",
      "british\n",
      "government\n",
      "cut\n",
      "exploratory\n",
      "research\n",
      "ai\n",
      "next\n",
      "year\n",
      "would\n",
      "later\n",
      "called\n",
      "ai\n",
      "winter\n",
      "period\n",
      "obtaining\n",
      "funding\n",
      "ai\n",
      "project\n",
      "difficult\n",
      "early\n",
      "ai\n",
      "research\n",
      "revived\n",
      "commercial\n",
      "success\n",
      "expert\n",
      "system\n",
      "form\n",
      "ai\n",
      "program\n",
      "simulated\n",
      "knowledge\n",
      "analytical\n",
      "skill\n",
      "human\n",
      "expert\n",
      "market\n",
      "ai\n",
      "reached\n",
      "billion\n",
      "dollar\n",
      "time\n",
      "japan\n",
      "fifth\n",
      "generation\n",
      "computer\n",
      "project\n",
      "inspired\n",
      "u\n",
      "british\n",
      "government\n",
      "restore\n",
      "funding\n",
      "academic\n",
      "research\n",
      "however\n",
      "beginning\n",
      "collapse\n",
      "lisp\n",
      "machine\n",
      "market\n",
      "ai\n",
      "fell\n",
      "disrepute\n",
      "second\n",
      "longer\n",
      "lasting\n",
      "winter\n",
      "began\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "for i in (corpus):\n",
    "    words = nltk.word_tokenize(i)\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            print(lemmatizer.lemmatize(word))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0c4bce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularlization and Limitizing the corpus\n",
    "\n",
    "import re\n",
    "corpus1=[]\n",
    "for i in range (len(sentences)):\n",
    "    review=re.sub('[^a-zA-Z]',' ',sentences[i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[lemmatizer.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review=' '.join(review)\n",
    "    corpus1.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f8f9bd3b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'second': 141,\n",
       " 'vision': 164,\n",
       " 'known': 86,\n",
       " 'connectionist': 34,\n",
       " 'approach': 6,\n",
       " 'sought': 149,\n",
       " 'achieve': 1,\n",
       " 'intelligence': 82,\n",
       " 'learning': 91,\n",
       " 'proponent': 126,\n",
       " 'prominently': 125,\n",
       " 'frank': 65,\n",
       " 'rosenblatt': 139,\n",
       " 'connect': 32,\n",
       " 'perceptron': 113,\n",
       " 'way': 165,\n",
       " 'inspired': 80,\n",
       " 'connection': 33,\n",
       " 'neuron': 106,\n",
       " 'james': 83,\n",
       " 'manyika': 98,\n",
       " 'others': 111,\n",
       " 'compared': 29,\n",
       " 'two': 163,\n",
       " 'mind': 102,\n",
       " 'symbolic': 156,\n",
       " 'ai': 3,\n",
       " 'brain': 21,\n",
       " 'argues': 7,\n",
       " 'dominated': 51,\n",
       " 'push': 128,\n",
       " 'artificial': 9,\n",
       " 'period': 114,\n",
       " 'due': 52,\n",
       " 'part': 112,\n",
       " 'intellectual': 81,\n",
       " 'tradition': 161,\n",
       " 'descartes': 45,\n",
       " 'boole': 19,\n",
       " 'gottlob': 74,\n",
       " 'frege': 66,\n",
       " 'bertrand': 17,\n",
       " 'russell': 140,\n",
       " 'based': 13,\n",
       " 'cybernetics': 40,\n",
       " 'neural': 105,\n",
       " 'network': 104,\n",
       " 'pushed': 129,\n",
       " 'background': 12,\n",
       " 'gained': 70,\n",
       " 'new': 107,\n",
       " 'prominence': 124,\n",
       " 'recent': 131,\n",
       " 'decade': 42,\n",
       " 'field': 61,\n",
       " 'research': 134,\n",
       " 'born': 20,\n",
       " 'workshop': 170,\n",
       " 'dartmouth': 41,\n",
       " 'college': 27,\n",
       " 'attendee': 11,\n",
       " 'became': 14,\n",
       " 'founder': 64,\n",
       " 'leader': 90,\n",
       " 'student': 152,\n",
       " 'produced': 119,\n",
       " 'program': 121,\n",
       " 'press': 116,\n",
       " 'described': 46,\n",
       " 'astonishing': 10,\n",
       " 'computer': 30,\n",
       " 'checker': 25,\n",
       " 'strategy': 151,\n",
       " 'solving': 148,\n",
       " 'word': 168,\n",
       " 'problem': 118,\n",
       " 'algebra': 4,\n",
       " 'proving': 127,\n",
       " 'logical': 94,\n",
       " 'theorem': 159,\n",
       " 'speaking': 150,\n",
       " 'english': 54,\n",
       " 'middle': 101,\n",
       " 'heavily': 76,\n",
       " 'funded': 68,\n",
       " 'department': 44,\n",
       " 'defense': 43,\n",
       " 'laboratory': 87,\n",
       " 'established': 55,\n",
       " 'around': 8,\n",
       " 'world': 171,\n",
       " 'researcher': 135,\n",
       " 'convinced': 36,\n",
       " 'would': 172,\n",
       " 'eventually': 56,\n",
       " 'succeed': 154,\n",
       " 'creating': 37,\n",
       " 'machine': 96,\n",
       " 'general': 71,\n",
       " 'considered': 35,\n",
       " 'goal': 73,\n",
       " 'herbert': 77,\n",
       " 'simon': 142,\n",
       " 'predicted': 115,\n",
       " 'capable': 24,\n",
       " 'within': 167,\n",
       " 'twenty': 162,\n",
       " 'year': 174,\n",
       " 'work': 169,\n",
       " 'man': 97,\n",
       " 'marvin': 100,\n",
       " 'minsky': 103,\n",
       " 'agreed': 2,\n",
       " 'writing': 173,\n",
       " 'generation': 72,\n",
       " 'substantially': 153,\n",
       " 'solved': 147,\n",
       " 'failed': 59,\n",
       " 'recognize': 132,\n",
       " 'difficulty': 48,\n",
       " 'remaining': 133,\n",
       " 'task': 158,\n",
       " 'progress': 122,\n",
       " 'slowed': 146,\n",
       " 'response': 136,\n",
       " 'criticism': 38,\n",
       " 'sir': 144,\n",
       " 'lighthill': 92,\n",
       " 'ongoing': 110,\n",
       " 'pressure': 117,\n",
       " 'congress': 31,\n",
       " 'fund': 67,\n",
       " 'productive': 120,\n",
       " 'project': 123,\n",
       " 'british': 22,\n",
       " 'government': 75,\n",
       " 'cut': 39,\n",
       " 'exploratory': 58,\n",
       " 'next': 108,\n",
       " 'later': 89,\n",
       " 'called': 23,\n",
       " 'winter': 166,\n",
       " 'obtaining': 109,\n",
       " 'funding': 69,\n",
       " 'difficult': 47,\n",
       " 'early': 53,\n",
       " 'revived': 138,\n",
       " 'commercial': 28,\n",
       " 'success': 155,\n",
       " 'expert': 57,\n",
       " 'system': 157,\n",
       " 'form': 63,\n",
       " 'simulated': 143,\n",
       " 'knowledge': 85,\n",
       " 'analytical': 5,\n",
       " 'skill': 145,\n",
       " 'human': 79,\n",
       " 'market': 99,\n",
       " 'reached': 130,\n",
       " 'billion': 18,\n",
       " 'dollar': 50,\n",
       " 'time': 160,\n",
       " 'japan': 84,\n",
       " 'fifth': 62,\n",
       " 'restore': 137,\n",
       " 'academic': 0,\n",
       " 'however': 78,\n",
       " 'beginning': 16,\n",
       " 'collapse': 26,\n",
       " 'lisp': 93,\n",
       " 'fell': 60,\n",
       " 'disrepute': 49,\n",
       " 'longer': 95,\n",
       " 'lasting': 88,\n",
       " 'began': 15}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()\n",
    "\n",
    "x=cv.fit_transform(corpus1)\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a1a9fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'second vision known connectionist approach sought achieve intelligence learning'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c08ce440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7f582d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde1c835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f8bcf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4d323143",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg2=\"\"\"Plastic is a non-biodegradable material.\n",
    "It does not mix with soil and reduces soil fertility.[\n",
    "It cannot be automatically recycled 'and harms the Earth’s life cycle. However, \n",
    "plastic is replacing the traditional use of jute, cotton or paper . . materials and \n",
    "is becoming a threat to the living world. Children run the risk of \n",
    "suffocation when playing with plastic bags. Coloured plastic carry-bags cause \n",
    "food poisoning. Plastic waste, if not disposed of carefully, \n",
    "often clogs sewage systems.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "97c6cb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in e:\\anaconda\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in e:\\anaconda\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in e:\\anaconda\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in e:\\anaconda\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\anaconda\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: colorama in e:\\anaconda\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "\n",
    "import nltk \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c4750e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Muhammad Talha\n",
      "[nltk_data]     Awan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# tokenization of the paragraph or corpus\n",
    "nltk.download('punkt')\n",
    "sentences2=nltk.sent_tokenize(pg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b24804f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Muhammad Talha\n",
      "[nltk_data]     Awan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Muhammad Talha\n",
      "[nltk_data]     Awan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "stemmer2=PorterStemmer()\n",
    "lemmatizer2=WordNetLemmatizer()\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "16d2033a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d416f963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plastic is a non-biodegradable material.\n",
      "plastic is a non biodegradable material \n",
      "['plastic', 'is', 'a', 'non', 'biodegradable', 'material']\n",
      "['plastic', 'non', 'biodegradable', 'material']\n",
      "plastic non biodegradable material\n",
      "[]\n",
      "--------------------- sentences completed ----- 0\n",
      "It does not mix with soil and reduces soil fertility.\n",
      "it does not mix with soil and reduces soil fertility \n",
      "['it', 'does', 'not', 'mix', 'with', 'soil', 'and', 'reduces', 'soil', 'fertility']\n",
      "['mix', 'soil', 'reduces', 'soil', 'fertility']\n",
      "mix soil reduces soil fertility\n",
      "['plastic non biodegradable material']\n",
      "--------------------- sentences completed ----- 1\n",
      "[\n",
      "It cannot be automatically recycled 'and harms the Earth’s life cycle.\n",
      "  it cannot be automatically recycled  and harms the earth s life cycle \n",
      "['it', 'cannot', 'be', 'automatically', 'recycled', 'and', 'harms', 'the', 'earth', 's', 'life', 'cycle']\n",
      "['cannot', 'automatically', 'recycled', 'harm', 'earth', 'life', 'cycle']\n",
      "cannot automatically recycled harm earth life cycle\n",
      "['plastic non biodegradable material', 'mix soil reduces soil fertility']\n",
      "--------------------- sentences completed ----- 2\n",
      "However, \n",
      "plastic is replacing the traditional use of jute, cotton or paper . . materials and \n",
      "is becoming a threat to the living world.\n",
      "however   plastic is replacing the traditional use of jute  cotton or paper     materials and  is becoming a threat to the living world \n",
      "['however', 'plastic', 'is', 'replacing', 'the', 'traditional', 'use', 'of', 'jute', 'cotton', 'or', 'paper', 'materials', 'and', 'is', 'becoming', 'a', 'threat', 'to', 'the', 'living', 'world']\n",
      "['however', 'plastic', 'replacing', 'traditional', 'use', 'jute', 'cotton', 'paper', 'material', 'becoming', 'threat', 'living', 'world']\n",
      "however plastic replacing traditional use jute cotton paper material becoming threat living world\n",
      "['plastic non biodegradable material', 'mix soil reduces soil fertility', 'cannot automatically recycled harm earth life cycle']\n",
      "--------------------- sentences completed ----- 3\n",
      "Children run the risk of \n",
      "suffocation when playing with plastic bags.\n",
      "children run the risk of  suffocation when playing with plastic bags \n",
      "['children', 'run', 'the', 'risk', 'of', 'suffocation', 'when', 'playing', 'with', 'plastic', 'bags']\n",
      "['child', 'run', 'risk', 'suffocation', 'playing', 'plastic', 'bag']\n",
      "child run risk suffocation playing plastic bag\n",
      "['plastic non biodegradable material', 'mix soil reduces soil fertility', 'cannot automatically recycled harm earth life cycle', 'however plastic replacing traditional use jute cotton paper material becoming threat living world']\n",
      "--------------------- sentences completed ----- 4\n",
      "Coloured plastic carry-bags cause \n",
      "food poisoning.\n",
      "coloured plastic carry bags cause  food poisoning \n",
      "['coloured', 'plastic', 'carry', 'bags', 'cause', 'food', 'poisoning']\n",
      "['coloured', 'plastic', 'carry', 'bag', 'cause', 'food', 'poisoning']\n",
      "coloured plastic carry bag cause food poisoning\n",
      "['plastic non biodegradable material', 'mix soil reduces soil fertility', 'cannot automatically recycled harm earth life cycle', 'however plastic replacing traditional use jute cotton paper material becoming threat living world', 'child run risk suffocation playing plastic bag']\n",
      "--------------------- sentences completed ----- 5\n",
      "Plastic waste, if not disposed of carefully, \n",
      "often clogs sewage systems.\n",
      "plastic waste  if not disposed of carefully   often clogs sewage systems \n",
      "['plastic', 'waste', 'if', 'not', 'disposed', 'of', 'carefully', 'often', 'clogs', 'sewage', 'systems']\n",
      "['plastic', 'waste', 'disposed', 'carefully', 'often', 'clog', 'sewage', 'system']\n",
      "plastic waste disposed carefully often clog sewage system\n",
      "['plastic non biodegradable material', 'mix soil reduces soil fertility', 'cannot automatically recycled harm earth life cycle', 'however plastic replacing traditional use jute cotton paper material becoming threat living world', 'child run risk suffocation playing plastic bag', 'coloured plastic carry bag cause food poisoning']\n",
      "--------------------- sentences completed ----- 6\n"
     ]
    }
   ],
   "source": [
    "# Regularlization and Limitizing the corpus\n",
    "\n",
    "import re\n",
    "corpus2=[]\n",
    "for i in range (len(sentences2)):\n",
    "    print(sentences2[i])\n",
    "    \n",
    "    review2=re.sub('[^a-zA-Z]',' ',sentences2[i])\n",
    "    review2=review2.lower()\n",
    "    print(review2)\n",
    "    \n",
    "    review2=review2.split()\n",
    "    print(review2)\n",
    "    \n",
    "    review2=[lemmatizer2.lemmatize(word) for word in review2 if not word in set(stopwords.words('english'))]\n",
    "    print(review2)\n",
    "    \n",
    "    review2=' '.join(review2)\n",
    "    print(review2)\n",
    "    \n",
    "    print(corpus2)\n",
    "    print(\"--------------------- sentences completed -----\",i)\n",
    "    corpus2.append(review2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "257a94f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'plastic': 27,\n",
       " 'non': 24,\n",
       " 'biodegradable': 3,\n",
       " 'material': 22,\n",
       " 'mix': 23,\n",
       " 'soil': 36,\n",
       " 'reduces': 31,\n",
       " 'fertility': 15,\n",
       " 'cannot': 4,\n",
       " 'automatically': 0,\n",
       " 'recycled': 30,\n",
       " 'harm': 17,\n",
       " 'earth': 14,\n",
       " 'life': 20,\n",
       " 'cycle': 12,\n",
       " 'however': 18,\n",
       " 'replacing': 32,\n",
       " 'traditional': 40,\n",
       " 'use': 41,\n",
       " 'jute': 19,\n",
       " 'cotton': 11,\n",
       " 'paper': 26,\n",
       " 'becoming': 2,\n",
       " 'threat': 39,\n",
       " 'living': 21,\n",
       " 'world': 43,\n",
       " 'child': 8,\n",
       " 'run': 34,\n",
       " 'risk': 33,\n",
       " 'suffocation': 37,\n",
       " 'playing': 28,\n",
       " 'bag': 1,\n",
       " 'coloured': 10,\n",
       " 'carry': 6,\n",
       " 'cause': 7,\n",
       " 'food': 16,\n",
       " 'poisoning': 29,\n",
       " 'waste': 42,\n",
       " 'disposed': 13,\n",
       " 'carefully': 5,\n",
       " 'often': 25,\n",
       " 'clog': 9,\n",
       " 'sewage': 35,\n",
       " 'system': 38}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()\n",
    "\n",
    "x=cv.fit_transform(corpus2)\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dc42f201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cannot automatically recycled harm earth life cycle'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "24cdbe62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[2].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bc45c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f1bfeb52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54edaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
